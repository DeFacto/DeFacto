[eval]
data-directory = /home/user/Repos/DeFacto/data/
train-directory = eval/train
test-directory = eval/test

[boa]
; how many boa patterns should be used to search: N * NUMBER_OF_BOA_PATTERNS queries and search results will be generated
NUMBER_OF_BOA_PATTERNS = 10
; only patterns which have a pattern score higher than this will be used to search for fact in text
PATTERN_SCORE_THRESHOLD = 0.5
languages = de,en,fr

[mysql]
; database host name
DB_HOST = localhost
; database port number
DB_PORT = 3306
; database's name
DB_NAME = dbpedia_metrics
; database user's name
DB_USER = user
; database user's password
DB_PWD = pass

[local_corpora]
; the set of indexed (input) fields
FIELDS_INPUT = titleText,text
; the set of indexed (output) fields
FIELDS_OUTPUT = id,titleText

wiki_service_en_url = http://139.18.2.164:8080/solr/wiki_en/
wiki_service_fr_url = http://139.18.2.164:8080/solr/wiki_fr/
wiki_service_de_url = http://139.18.2.164:8080/solr/wiki_de/
wiki_fields_in = titleText,text
wiki_fields_out = id,titleText

[crawl]
; the bing api key get your own at: 
; BING_API_KEY = vm+K+uxrwn3oRap0Ou87KePKjGzH3gLGFM1vMw776wc=
BING_API_KEY = grgZ1JV8G6y/zbIVA9DChAuOuUIjdcsHoxhTkz2MXKQ
; number of search results return be the search engine
NUMBER_OF_SEARCH_RESULTS = 20
; number of search results returned by the search engine
NUMBER_OF_SEARCH_RESULTS_LC = 20
; thredas
NUMBER_OF_SEARCH_RESULTS_THREADS = 40
; crawl thread gets terminated after this time
WEB_SEARCH_TIMEOUT_MILLISECONDS = 10000
; One thread for every BOA pattern, usually the same as the number of BOA patterns 
NUMBER_OF_SEARCH_THREADS = 10
; the amount of time which lies between a google page rank request (in ms)
GOOGLE_WAIT_TIME = 2000
; get the page rank or not
getPageRank = true
; folder where intermediate crawl data is stored
CRAWL_STORAGE_FOLDER = /home/user/Repos/DeFacto/data/crawl_repo
; number of crawlers
NUMBER_OF_CRAWLERS = 7

solr_searchresults	= http://139.18.2.164:8080/solr/factbench_searchresults/
solr_topicterms	    = http://139.18.2.164:8080/solr/factbench_topicterms/
solr_boa_fr = http://139.18.2.164:8080/solr/boa_fr/
solr_boa_de = http://139.18.2.164:8080/solr/boa_de/
solr_boa_en = http://139.18.2.164:8080/solr/boa_en/


[server]
ip = http://139.18.2.164/
port = 1234


[extract]
; when we search for sentence which contain two labels, this is the amount of tokens between them
NUMBER_OF_TOKENS_BETWEEN_ENTITIES = 20
; the number of stanford models which get loaded on system start
NUMBER_NLP_STANFORD_MODELS = 2

[fact]
; write the training examples in this file
FACT_TRAINING_DATA_FILENAME = resources/training/arff/fact/defacto_fact.arff
; LINEAR_REGRESSION_CLASSIFIER or MULTILAYER_PERCEPTRON or SMO
FACT_CLASSIFIER_TYPE = machinelearning/model/fact/66_33_proof_smo_reg/66_33_proof_smo_reg_polykernel.model
ARFF_TRAINING_DATA_FILENAME = machinelearning/model/fact/66_33_proof_smo_reg/66_33_proof_smo_reg_polykernel.arff
; do we want to write the fact confirmation weka training file
OVERWRITE_FACT_TRAINING_FILE = false


[evidence]
; LINEAR_REGRESSION_CLASSIFIER or MULTILAYER_PERCEPTRON or SMO
EVIDENCE_CLASSIFIER_TYPE = machinelearning/model/evidence/j48_ml_.model
; write the training examples in this file
EVIDENCE_TRAINING_DATA_FILENAME = resources/training/arff/evidence/defacto_evidence.arff
; active this flag to rewrite the training data
OVERWRITE_EVIDENCE_TRAINING_FILE = false
; see the paper Nakamura et. al. 2007 for details 
WEBSITE_SIMILARITY_THRESHOLD = 0.8
; onyl this much results will be returned from wikipedia topic term query: "barack obama michelle obama" returns 860 results
MAX_WIKIPEDIA_RESULTS = 10
; use only the n most frequent topic terms from wikipedia pages
TOPIC_TERMS_MAX_SIZE = 10
; one topic term needs to be in more than this pages
TOPIC_TERM_PAGE_THRESHOLD = 1
; set this flag if you want to override the current trained model
OVERWRITE_EVIDENCE_MODEL = false
; no google page rank defined yet
UNASSIGNED_PAGE_RANK = 11
; threshold for when a complex proof is considered to be supporting a fact
CONFIRMATION_THRESHOLD = 0.5
;return all websites even the website has no proof
DISPLAY_WEBSITES_WITH_NO_PROOF = false

WORDNET_DICTIONARY = /home/user/Repos/DeFacto/data/wordnet/dict

[settings]
; this is used to get the labels of the resources from the training models
RESOURCE_LABEL = http://www.w3.org/2000/01/rdf-schema#label
; training mode or demo mode (training generates arff files, demo mode outputs confidence scores)
TRAINING_MODE = false
; time period searcher: occurrence, global, domain
TIME_PERIOD_SEARCHER = domain
; tiny, small, medium, large
context-size = tiny
; frequency or pattern
periodSearchMethod = frequency
